# ROS2 Polygon Navigation System - Architecture

## ğŸ“Š System Overview

Dieses Dokument beschreibt die Architektur des ROS2 Polygon Navigation Systems mit allen Nodes, Topics und deren VerknÃ¼pfungen.

## ğŸ–¼ï¸ Visual Diagram

Das visuelle Architektur-Diagramm ist in `ros2_architecture.png` zu finden.

## ğŸ“¦ Nodes

### 1. yolo_processor
**Node Name:** `yolo_processor`  
**Python File:** `polygon_nav/polygon_nav/yolo_processor.py`

**Subscribers:**
- `/image` (sensor_msgs/Image) - Kamera-Bildinput

**Publishers:**
- `/yolo/detections` (vision_msgs/Detection2DArray) - Personendetektionen mit Tracking-IDs
- `/yolo/processed_image` (sensor_msgs/Image) - Visualisiertes Bild mit Bounding Boxes

**Funktion:**
- Verwendet YOLO v8 fÃ¼r Personendetektion
- BYTETrack fÃ¼r robustes Multi-Object-Tracking
- Publiziert erkannte Personen mit persistenten Tracking-IDs
- Loggt Detections in JSON-Dateien

---

### 2. hand_gesture_detector
**Node Name:** `hand_gesture_detector`  
**Python File:** `polygon_nav/polygon_nav/hand_gesture_detector.py`

**Subscribers:**
- `/image` (sensor_msgs/Image) - Kamera-Bildinput

**Publishers:**
- `/hand/pointer_finger` (geometry_msgs/PointStamped) - Zeigefinger-Position (x, y, z)
- `/hand/annotated_image` (sensor_msgs/Image) - Bild mit Hand-Skeleton

**Funktion:**
- MediaPipe Hand-Tracking fÃ¼r Hand-Erkennung
- ÃœberprÃ¼ft ob Zeigefinger gehoben ist
- Publiziert Koordinaten wenn Zeigefinger erkannt wird

---

### 3. person_position_tracker
**Node Name:** `person_position_tracker`  
**Python File:** `polygon_nav/polygon_nav/person_position_tracker.py`

**Subscribers:**
- `/yolo/detections` (vision_msgs/Detection2DArray) - Personendetektionen
- `/odom` (nav_msgs/Odometry) - Robot-Position und Orientierung
- `/scan` (sensor_msgs/LaserScan) - LiDAR-Daten

**Publishers:**
- `/person_position` (geometry_msgs/PointStamped) - Welt-Position der Person
- `/person_marker` (geometry_msgs/PoseStamped) - Marker fÃ¼r RViz

**Funktion:**
- Berechnet Welt-Koordinaten der erkannten Personen
- Verwendet Kamera-Detection, LiDAR-Distanz und Odometrie
- Transformiert relative Position in Welt-Koordinaten (odom frame)

---

### 4. polygon_navigation_client
**Node Name:** `polygon_navigation_client`  
**Python File:** `polygon_nav/polygon_nav/polygon_nav_client.py`

**Action Client:**
- `navigate_to_pose` (nav2_msgs/action/NavigateToPose) - NAV2 Navigation Action

**Funktion:**
- Implementiert Polygon-basierte Coverage-Pfade
- Sendet Navigations-Ziele an Nav2
- Erstellt boustrophedonischen Pfad fÃ¼r vollstÃ¤ndige Abdeckung

---

## ğŸ”„ Topic Flow

```
Camera/Lidar â†’ /image, /odom, /scan
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXTERNAL DATA SOURCES              â”‚
â”‚  â€¢ Camera Stream                     â”‚
â”‚  â€¢ Odometry                          â”‚
â”‚  â€¢ LiDAR Scan                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DETECTION LAYER                    â”‚
â”‚  â€¢ yolo_processor                   â”‚
â”‚    â†’ /yolo/detections               â”‚
â”‚    â†’ /yolo/processed_image          â”‚
â”‚  â€¢ hand_gesture_detector            â”‚
â”‚    â†’ /hand/pointer_finger           â”‚
â”‚    â†’ /hand/annotated_image          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRACKING LAYER                     â”‚
â”‚  â€¢ person_position_tracker          â”‚
â”‚    â† /yolo/detections               â”‚
â”‚    â† /odom                           â”‚
â”‚    â† /scan                           â”‚
â”‚    â†’ /person_position                â”‚
â”‚    â†’ /person_marker                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NAVIGATION LAYER                   â”‚
â”‚  â€¢ polygon_navigation_client         â”‚
â”‚    â†’ navigate_to_pose Action         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Message Types

### Input Topics (External)
| Topic | Type | Description |
|-------|------|-------------|
| `/image` | sensor_msgs/Image | Kamera-Bildstream |
| `/odom` | nav_msgs/Odometry | Robot-Odometrie und Orientierung |
| `/scan` | sensor_msgs/LaserScan | LiDAR-Scandaten |

### Output Topics (Internal)
| Topic | Type | Publisher | Description |
|-------|------|-----------|-------------|
| `/yolo/detections` | vision_msgs/Detection2DArray | yolo_processor | Personendetektionen mit Tracking |
| `/yolo/processed_image` | sensor_msgs/Image | yolo_processor | Visualisiertes YOLO-Bild |
| `/hand/pointer_finger` | geometry_msgs/PointStamped | hand_gesture_detector | Zeigefinger-Koordinaten |
| `/hand/annotated_image` | sensor_msgs/Image | hand_gesture_detector | Bild mit Hand-Skeleton |
| `/person_position` | geometry_msgs/PointStamped | person_position_tracker | Welt-Position der Person |
| `/person_marker` | geometry_msgs/PoseStamped | person_position_tracker | RViz Marker |

## ğŸš€ Usage

### Start Detection Nodes
```bash
# Terminal 1: YOLO Processor
ros2 run polygon_nav yolo_processor

# Terminal 2: Hand Gesture Detector
ros2 run polygon_nav hand_gesture_detector

# Terminal 3: Person Position Tracker
ros2 run polygon_nav person_tracker
```

### Visualize in RViz
```bash
ros2 run rviz2 rviz2
```

**Display Topics:**
- `/yolo/processed_image` - YOLO Visualisierung
- `/hand/annotated_image` - Hand-Tracking Visualisierung
- `/person_marker` - Person Position Marker

### Monitor Topics
```bash
# Zeigefinger-Position
ros2 topic echo /hand/pointer_finger

# Person Positionen
ros2 topic echo /person_position

# YOLO Detections
ros2 topic echo /yolo/detections
```

## ğŸ”§ Dependencies

### System Packages
- `ros-humble-desktop`
- `python3-opencv`
- `cv_bridge`
- `sensor_msgs`
- `geometry_msgs`
- `vision_msgs`
- `nav2_msgs`

### Python Packages
- `ultralytics` (YOLO)
- `mediapipe` (Hand Tracking)
- `numpy`
- `cv_bridge`
- `shapely` (Polygon Navigation)

## ğŸ“ Notes

- **Coordinate Frames:** Alle Positionen werden im `odom` frame berechnet
- **Tracking IDs:** YOLO BYTETrack vergibt persistente IDs fÃ¼r erkannte Personen
- **Real-time Processing:** Alle Nodes arbeiten asynchron und unabhÃ¤ngig
- **Modularity:** Jeder Node kann einzeln gestartet/gestoppt werden

## ğŸ¯ Workflow

1. **Image Input** â†’ Camera publiziert auf `/image`
2. **YOLO Detection** â†’ Erkennt Personen, vergibt Tracking-IDs
3. **Hand Detection** â†’ Erkennt Zeigefinger-Gesten
4. **Position Tracking** â†’ Berechnet Welt-Koordinaten der Personen
5. **Navigation** â†’ Navigiert zu Zielpositionen

